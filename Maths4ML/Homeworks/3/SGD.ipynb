{"cells":[{"cell_type":"markdown","metadata":{},"source":["# stochastic gradient descent (SGD)\n","using batches to fasten gradient computation (approximation)\n","\n","SGD has to be implemented with static stepsize"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def l(w,D): # D is the data, w is the new x\n"," #loss function over D\n"," return\n","\n","def grad(w, D):\n"," #gradient of SGD (sommatoria dei gradienti degli indici del batch [subset M])\n"," #this is an approximation of the gradiant, it isn't for sure the direction of steepest descent\n","\n","def SGD(l, grad_l, w0, D, batch_size, n_epochs):\n"," #same outputs as GD\n"," #initialization\n"," a=1 #fixed\n"," # D =(X,Y) where X is d x N \n"," # Y is N\n"," X, Y= D\n"," d,N=X.shape\n"," n_batch_per_epoch=...\n"," #iterations\n"," k=0\n"," f_val[0]=x0\n"," conditions=True\n"," X_backup=X\n"," Y_backup=Y\n"," #loop\n"," for epoch in range(n_epochs):\n"," for k in range (n_batch_per_epoch):\n"," #sample M from D\n"," Mx #batch di X\n"," My #batch di Y\n"," M=(Mx, My)\n"," #remove Mx and My from X and Y\n"," X=...\n"," Y=...\n"," #update w\n"," w=w0-a*grad(M)\n"," #restart \n"," w0=w\n"," #reload X and Y\n"," X=X_backup\n"," Y=Y_backup\n"," #NOTA: YOU HAVE TO SHUFFLE AGAIN AFTER EACH EPOCH!!!! \n"," #so we can sample each time different batches\n"," #reshuffle\n"," return w\n"," #REMEMBER: w0 in SGD should be chosen randomly (sample from Gaussian), and not fixed\n"," #(upgrade): modify the code above to return f_val (over M)\n"," #f_val[k]=l(w_k, M)\n"," \n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"4723fcde35915bce623ab93bc635d3362c8c73e8db680d81579b810979dbf657"}}},"nbformat":4,"nbformat_minor":2}
