{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch with cross validation\n",
    "the function `GridSearchCV` iterates a cross validation experiment to train and test a model with different combinations of paramater values\n",
    "- for each parameter we set a list of values to test, the function will generate all the combinations\n",
    "- we choose a *score function* which will be used for the optimization\n",
    "    - e.g. `accuracy_score`, `precision_score`, `cohen_kappa_score`, `f1_score`, see this [page](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) for reference\n",
    "- the **output is a dictionary** containing \n",
    "    - the set of parameters which maximize the score \n",
    "    - the test scores\n",
    "\n",
    "The parameters to feed in the grid search are in a list of dictionaries, grid search will try all the possible combinations, using crossvalidation (by default, it uses 5-fold cross validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "report=[]\n",
    "print(\"========================================\")\n",
    "print(\"# Tuning hyper-parameters for precision\")\n",
    "print()\n",
    "i=0\n",
    "for s in scores:\n",
    "    report.append({})\n",
    "    for m in models: #note: m is the key, not the whole dictionary\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Trying model\",models[m]['name'])\n",
    "        model = GridSearchCV(models[m]['estimator'], models[m]['param'], scoring=s)\n",
    "        model.fit(X_train, y_train)\n",
    "        print_results(model)\n",
    "        report[i][models[m]['name']]=model.best_score_\n",
    "    i+=1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to access the reasults:\n",
    "\n",
    "`model.best_params_`<br>\n",
    "`model.cv_results_['mean_test_score']`<br>`\n",
    "model.cv_results_['std_test_score']`<br>\n",
    "`model.cv_results_['params']`\n",
    "\n",
    "Here's a function to display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(model):\n",
    "    print(\"Best parameters set found on train set:\")\n",
    "    print()\n",
    "    # if best is linear there is no gamma parameter\n",
    "    print(model.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on train set:\")\n",
    "    print()\n",
    "    means = model.cv_results_['mean_test_score']\n",
    "    stds = model.cv_results_['std_test_score']\n",
    "    params = model.cv_results_['params']\n",
    "    for mean, std, params_tuple in zip(means, stds, params):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params_tuple))\n",
    "    print()\n",
    "    print(\"Detailed classification report for the best parameter set:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full train set.\")\n",
    "    print(\"The scores are computed on the full test set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1 (main, Jan  6 2023, 00:00:00) [GCC 12.2.1 20221121 (Red Hat 12.2.1-4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
