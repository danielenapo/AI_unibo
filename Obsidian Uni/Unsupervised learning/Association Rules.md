_Method to show the probability of relationships between data items, within large data sets in various types of databases_ 

# Definitions
- **Support** -> frequency of occurrence of an itemset over the whole dataset
- **Confidence** -> Support of a subgroup divided by support of a supergroup
	![[Pasted image 20230126175712.png]]
- **ASSOCIATION RULE** -> expression $A \implies C$ where A and C are itemsets

# Mining Association rules
there are two steps for getting the final rules
1. ==**Frequent itemset generation**== -> generate all itemsets whose **support** is greater than **minsup** (threshold value, [[Hyperparameter]])
2. ==**Rule generation**== -> generate high **confidence** rules from each frequent itemset

## Frequent itemset generation 
To find all association rules, one could try all combinations of itemsets, but we would have $2^{D}$ itemsets (With D items)
In this example, we have only 4 items. (What if we have hundreds of thousands?)
![[Pasted image 20230126175859.png]]
This graph is obtained just with 5 individual items (32 itemsets in total).
We need a way to not generate all possible itemset, but getting all the ones with support > minsup

## Apriori algorithm
Following the ==**apriori principle**, the support of an itemset never exceeds the support of all its subsets.== (anti-monotone property of support)
![[Pasted image 20230126181111.png]]
With this information, we could prune our graph a priori:
![[Pasted image 20230126181139.png]]
Pruning example with minsup=3
![[Pasted image 20230126181310.png]]
## Rule generation
Rule generation focuses on confidence instead of support (finds high-confidecnce rules)
But confidence doesen't have anti-monotone properties on subsets like support.
conf(ABC --> D) could be larger or smaller than conf(AB --> D)

But considering different rules generated by the same itemset, the following anti-monotone property holds:
![[Pasted image 20230126182237.png]]
==cofidence is antimonotone w.r.t. the number of items on the RHS of the rule.==
(decreases when moving an item or more from the left to the right hand)
![[Pasted image 20230126182506.png]]


# Statistical-based measures
Confidence alone can be misleading in some cases
![[Pasted image 20230126182605.png]]

In general, confidence is used as a base method, then analyzed with other statistical-based methods:
- **lift** -> ratio of true cases wrt independence
	1 if the items are independent
![[Pasted image 20230126182741.png]]
- **leverage** -> number of additional cases wrt independence
	0 if items are independent
![[Pasted image 20230126182813.png]]
- **conviction** (or novelty) -> frequency that rules makes an incorrect prediction
	infinite if the rule is always true
![[Pasted image 20230126182942.png]]

![[Pasted image 20230126183036.png]]

