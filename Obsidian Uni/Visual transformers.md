Transformers are a [[Neural Networks (MLP)]] architecture, originally developed for [[Natural Language Processing]] (automatic translation) by Google, in the paper "Attention is all you need" (2017).
Transformers only rely on Attention blocks, that exploit key-query-value correlations to attend to specific information in the input. 
It uses an autoregressive prediction mechanism