{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielenapo/AI_unibo/blob/main/Deep%20Learning%20/exams/gan_image_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylMADpelOs4i"
      },
      "source": [
        "# Generating Images with Generative Adversarial Networks (GANs)\n",
        "\n",
        "The purpose of the project is to test the ability of Generative Adversial Networks (GANs) in generating realistic-looking images. \n",
        "\n",
        "## Dataset\n",
        "\n",
        "The dataset used will be FashionMNIST. It contains low resolution ($28 \\times 28$) grey-scale images representing different kind of clothes. The dataset is available on keras and accessable in $\\texttt{tf.keras.datasets.fashion\\_mnist}$. Note that the pixel values for the images are initially in the interval $[0, 255]$. It is required to normalize them since all of the algorithm we will use require them to be in that format. To be fair, you will find the dataset already normalized, do not modify that part of the code.\n",
        "\n",
        "## Metrics\n",
        "\n",
        "Measuring the quality of newly generated images is a non-trivial task. Indeed, there is no label associated to each image, and thus it is impossible to measure the quality image-by-image. For that reason, common metrics uses statistical consideration on a generated dataset to test how well the network recovered the statistics of the original data. One of the most common is the Fr√©chet Inception Distance (FID). The idea of FID is that in a realistic-looking dataset of images, the statistics of the activation of the last hidden layer in a well-trained classificator should be similar to that of a dataset containing real images. Specifically, regarding FID, the Inception-v3 network is used as a classificator. A real dataset $\\mathbb{D}_r$ and a generated dataset $\\mathbb{D}_g$ are processed by the network, and the activation of the last hidden layer has mean and variance $(\\mu_r, \\Sigma_r)$, $(\\mu_g, \\Sigma_g)$ respectively. Then, FID is computed as:\n",
        "\n",
        "$$\n",
        "    FID(\\mathbb{D}_r, \\mathbb{D}_g) = || \\mu_r - \\mu_g ||^2 + Tr(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\ast \\Sigma_g)^{\\frac{1}{2}}) \n",
        "$$\n",
        "\n",
        "A Python implementation of FID can be found in the file $\\texttt{fid.py}$ that you find attached on Virtuale. Its usage is very simple, just generate $10k$ fake images with your GAN, and with the command $\\texttt{fid.get\\_fid(x\\_test, x\\_gen)}$, where $\\texttt{x\\_test}$ is the test set, containing $10k$ real images, you get the value for the FID of your network. Remember that, when passed through that function, $\\texttt{x\\_gen}$ **must** be a dataset of $10k$ images, in the interval $[0, 1]$. The number of $10k$ images is fundamental, since the value of FID strongly depends on the number of input images.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "You are required to implement a vanilla Generative Adversarial Network (GAN), not a variant of it (e.g. PixelGAN, CycleGAN, ... are **not** accepted). The maximum number of parameters is *15 million*, and every pre-trained network can be used as an add-on (the number of parameters for pre-trained network does not count). Clearly, only the training set can be used to train the network, no additional images (Data Augmentation is ok)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xMXzZm5FOs4o"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8i9lsJUOs4q"
      },
      "source": [
        "The images are normalized in $[0, 1]$. For simplicity, images are padded to have dimension $32 \\times 32$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIBKGLh7Os4r",
        "outputId": "5c633950-5551-49da-ead0-b67bb1a14f0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Training shape: (60000, 32, 32, 1), Training pixel values: (0.0, 1.0)\n",
            "Test shape: (10000, 32, 32, 1), Test pixel values: (0.0, 1.0)\n"
          ]
        }
      ],
      "source": [
        "# Load the data. Note that the labels y_train and y_test are not loaded since not required.\n",
        "(x_train, _), (x_test, _) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize and pad the datasets\n",
        "x_train = np.pad(x_train, ((0,0), (2,2), (2,2)))\n",
        "x_train = np.reshape(x_train, x_train.shape + (1, ))\n",
        "x_train = x_train / 255.\n",
        "\n",
        "x_test = np.pad(x_test, ((0,0), (2,2), (2,2)))\n",
        "x_test = np.reshape(x_test, x_test.shape + (1, ))\n",
        "x_test = x_test / 255.\n",
        "\n",
        "print(f\"Training shape: {x_train.shape}, Training pixel values: {x_train.min(), x_train.max()}\")\n",
        "print(f\"Test shape: {x_test.shape}, Test pixel values: {x_test.min(), x_test.max()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogDMGClBOs4r"
      },
      "source": [
        "Now, we import the functions for the computation of the FID, and we test that FID(x_train, x_test) is low.\n",
        "\n",
        "_Note: Computing the FID function requires some minutes. Consequently, it is suggested to comment this cell after you tested once, to reduce the execution time of the notebook. To speed-up the process, after a first use, the function will generate a file containing the value of the activations of the test set, so that it does not have to compute it again every time._ \n",
        "\n",
        "**Remember that, when you use the FID function, the first input MUST be the test set, while the second will be the generated images set.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZxbyJIcOs4s"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Do not modify this code. This is just for utilities.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# prepare the inception v3 model\n",
        "model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3), weights='imagenet')\n",
        "\n",
        "def get_inception_activations(inps, batch_size=100):\n",
        "    \"\"\"\n",
        "    Compute the activation for the model Inception v3 for a given input 'inps'.\n",
        "\n",
        "    Note: inps is assumed to be normalized in [0, 1].\n",
        "    \"\"\"\n",
        "    n_batches = inps.shape[0] // batch_size\n",
        "\n",
        "    act = np.zeros([inps.shape[0], 2048], dtype=np.float32)\n",
        "    for i in range(n_batches):\n",
        "        # Load a batch of data\n",
        "        inp = inps[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "        # Resize each image to match the input shape of Inception v3\n",
        "        inpr = tf.image.resize(inp, (299, 299))\n",
        "\n",
        "        # Resize images in the interval [-1, 1], given that inpr is in [0, 1].\n",
        "        inpr = inpr * 2 - 1 \n",
        "\n",
        "        # Predict the activation\n",
        "        act[i * batch_size:(i + 1) * batch_size] = model.predict(inpr, steps=1)\n",
        "\n",
        "        print(f\"Processed {str((i + 1) * batch_size)} images.\")\n",
        "    return act\n",
        "\n",
        "\n",
        "def get_fid(images1, images2):\n",
        "    \"\"\"\n",
        "    Compute the FID between two sets of images.\n",
        "\n",
        "    Note: it can take several minutes.\n",
        "    \"\"\"\n",
        "    from scipy.linalg import sqrtm\n",
        "\n",
        "    shape = np.shape(images1)[1]\n",
        "    print(\"Computing FID for {} dimensional images\".format(images1.shape))\n",
        "\n",
        "    # Inception v3 requires the input to have 3 channel. If this is not the\n",
        "    # case, just copy the same channel three times.\n",
        "    if images1.shape[-1] == 1:\n",
        "        images1 = np.concatenate([images1, images1, images1], axis=-1)\n",
        "        images2 = np.concatenate([images2, images2, images2], axis=-1)\n",
        "\n",
        "    # activation for true images is always the same: we just compute it once\n",
        "    if os.path.exists(\"act_mu.npy\"):\n",
        "        mu1 = np.load(\"act_mu.npy\")\n",
        "        sigma1 = np.load(\"act_sigma.npy\")\n",
        "    else:\n",
        "        act1 = get_inception_activations(images1)\n",
        "        mu1, sigma1 = act1.mean(axis=0), np.cov(act1, rowvar=False)\n",
        "        np.save(\"act_mu.npy\", mu1)\n",
        "        np.save(\"act_sigma.npy\", sigma1)\n",
        "    print('Done stage 1 of 2')\n",
        "\n",
        "    act2 = get_inception_activations(images2)\n",
        "    mu2, sigma2 = act2.mean(axis=0), np.cov(act2, rowvar=False)\n",
        "    print('Done stage 2 of 2')\n",
        "\n",
        "    # calculate sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
        "\n",
        "    # compute sqrt of product between cov\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "    # check and correct imaginary numbers from sqrt\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    # calculate score\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuXjjmh8Os4t"
      },
      "outputs": [],
      "source": [
        "# Compute the FID between the Test set and (the first 10k images of) Train set (should be low)\n",
        "train_fid = get_fid(x_test, x_train[:10_000])\n",
        "\n",
        "# Print out the results\n",
        "print(f\"FID(x_test, x_train) = {train_fid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnoGNCySOs4u"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, BatchNormalization, LeakyReLU, Flatten\n",
        "\n",
        "def generator(img_shape, latent_dim):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(512))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1024))\n",
        "  model.add(BatchNormalization(momentum=0.8))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "lolgatsWRXI6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator"
      ],
      "metadata": {
        "id": "nRj9MDqmSB8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator(img_shape):\n",
        "  model = Sequential()\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(512))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(256))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "wzDtamqFSEAL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN class"
      ],
      "metadata": {
        "id": "cWovBOeNSGPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "\n",
        "class GAN(Model):\n",
        "  def __init__(self, generator, discriminator):\n",
        "    super(GAN, self).__init__()\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "    self.discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "    z = Input(shape=(latent_dim,))\n",
        "    img = self.generator(z)\n",
        "\n",
        "    self.discriminator.trainable = False\n",
        "\n",
        "    validity = self.discriminator(img)\n",
        "\n",
        "    self.combined = Model(z, validity)\n",
        "    self.combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "  def train_step(self, data):\n",
        "    imgs, _ = data\n",
        "\n",
        "    valid = tf.ones((imgs.shape[0], 1))\n",
        "    fake = tf.zeros((imgs.shape[0], 1))\n",
        "\n",
        "    noise = tf.random.normal((imgs.shape[0], latent_dim))\n",
        "    gen_imgs = self.generator(noise)\n",
        "\n",
        "    d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
        "    d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
        "    d_loss = 0.5 * tf.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "    noise = tf.random.normal((imgs.shape[0], latent_dim))\n",
        "    g_loss = self.combined.train_on_batch(noise, valid)\n",
        "\n",
        "    return {'d_loss': d_loss, 'g_loss': g_loss}\n"
      ],
      "metadata": {
        "id": "KT3FwlLCSJae"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "70S98ZfVUQDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "latent_dim=100\n",
        "img_shape=(32,32,1)\n",
        "# Create the generator and discriminator\n",
        "generator = generator(img_shape, latent_dim)\n",
        "discriminator = discriminator(img_shape)\n",
        "\n",
        "# Create the GAN\n",
        "gan = GAN(generator, discriminator)\n",
        "\n",
        "# Train the GAN\n",
        "gan.fit(X_train, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "08YwNEG3UMBX",
        "outputId": "e62bb9ab-6cef-4b07-ef16-19534d0cbaca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b880d866b620>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create the generator and discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    220\u001b[0m             \u001b[0;34mf'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;34mf\" but it received {len(inputs)} input tensors. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Layer \"sequential\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor: shape=(), dtype=int32, numpy=32>, <tf.Tensor: shape=(), dtype=int32, numpy=32>, <tf.Tensor: shape=(), dtype=int32, numpy=1>]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.11 ('ml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2db16da740494d35c7f3749582ea0fec8725a5f0d9e976cf583dd1041e9a5f0f"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}